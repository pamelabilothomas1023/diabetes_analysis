import sys
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql.functions import lit, concat, min, floor, concat_ws, collect_list, max
from pyspark.ml.feature import QuantileDiscretizer
from pyspark.sql.types import *

def ReturnQuartile(df, cols):
    #This function groups a column into quartiles.  The input is cols, which is a list of columns
    for c in cols:
        #Create the quartiles
        discretizer = QuantileDiscretizer(numBuckets = 4, inputCol = c, outputCol = c + "bin", relativeError=0.01, handleInvalid = "error")
        #Apply the quartiles
        df = discretizer.fit(df).transform(df)
    return df

def func():
    spark = SparkSession.builder.appName("clin_var").getOrCreate()
    sc = spark.sparkContext
    df = spark.read.option("header", "true").option("inferschema", "true").csv("/mnt/data0/input/t2d_subgroups/t2d_clin_var.csv")
    #Read in the study ids generated by read_diagnosis_data
    studyidstest = spark.read.option("header", "true").option("inferschema", "true").csv(str(sys.argv[1]) + "_studyids_test.csv")
    studyidstrain = spark.read.option("header", "true").option("inferschema", "true").csv(str(sys.argv[1]) + "_studyids_train.csv")
    #Read the limit generated by read_diagnosis_data and conver tto float
    limitfile = open(str(sys.argv[1]) + "limitfile.txt", "r")
    limit = limitfile.read()
    limit = float(limit)
    limitfile.close()
    #Read the median generated by read_diagnosis_data and convert to float
    #these are the "important variables" that Scott defined
    importantvars = ["non_hdl_c", "ldl_hdl_ratio", "tsh", "fib_4_index", "total_cholesterol", "ldl_c", "hdl_c", "cholesterol_ratio", "total_bilirubin", "basophil_pc", "monocyte_count", "apri", "neutrophil_count", "albumin", "alp", "ast_alt_ratio", "eosinophil_pc", "protein", "hba1c", "alt", "egfr", "ast", "lymphocyte_pc", "calcium", "red_blood_pc", "platelet_count", "mcv", "mch", "glucose", "bun", "chloride", "creatinine", "co2"]
    #Quartile the variables
    df = ReturnQuartile(df, importantvars)
    i = 0
    #This loop applies the results of qdf to the dataframe
    while (i < len(importantvars)):
        #Get the not null values
        innerdf = df.where(df[importantvars[i] + "bin"].isNotNull())
        #Create the variables
        innerdf = innerdf.select(innerdf["STUDYID"], innerdf["DAYS_VIS_INDEX"].alias("DAYS_INDEX"), concat(innerdf[importantvars[i] + "bin"], lit("_"), lit(importantvars[i][0:15]), lit("_VAR")).alias("CODE"))
        #Get the year that the lab result was given in
        innerdf = innerdf.withColumn("DAYS_INDEX", floor((innerdf["DAYS_INDEX"].cast(FloatType()))/365))
        #Group the lab results together - this is to find out the most common lab quartile result over a given year
        innerdf = innerdf.groupby(innerdf['STUDYID'], innerdf['CODE'], innerdf['DAYS_INDEX']).count()
        #Find the largest count for each studyID and year (code should be the same)
        innerdfmax = innerdf.groupby(innerdf['STUDYID'], innerdf['DAYS_INDEX']).agg(max(innerdf['count']))
        #Rename the columns
        innerdfmax = innerdfmax.select(innerdfmax['STUDYID'], innerdfmax['DAYS_INDEX'], innerdfmax['max(count)'].alias('count'))
        #Join the columns such that only the ones that appear the most in a given year are included
        innerdf = innerdf.join(innerdfmax, on=['STUDYID', 'DAYS_INDEX', 'count'], how='inner')
        #Select the columns
        innerdf = innerdf.select(innerdf['STUDYID'], innerdf['CODE'], innerdf['DAYS_INDEX'])
        #Find the first time that a diagnosis appeared
        innerdf = innerdf.groupby([innerdf['STUDYID'], innerdf["CODE"]]).agg(min(innerdf['DAYS_INDEX']))
        #Rename columns
        innerdf = innerdf.select(innerdf['STUDYID'], innerdf['CODE'], innerdf['min(DAYS_INDEX)'].alias('DAYS_INDEX'))
        #Filter data such that the only data that remains occur after diagnosis date and before the median date
        innerdf = innerdf.where((innerdf['DAYS_INDEX'] == 0))
        #Count how many people receive each diagnosis
        innerdfcount = innerdf.groupby(innerdf['CODE']).count()
        #Filter out those diagnoses that occur in less than one percent of the population
        innerdfcount = innerdfcount.where(innerdfcount['COUNT'] > limit)
        #Get the codes that occur in greater than one percent of the population
        codes = innerdfcount.select('CODE').distinct().rdd.flatMap(lambda x: x).collect()
        #Filter the dataframe such that we only have those that contain the popular codes
        innerdf = innerdf.filter(innerdf['CODE'].isin(codes))
        #Get the dataframe for testing purposes
        testinnerdf = innerdf.join(studyidstest, on=['STUDYID'], how='inner')
        #Get the dataframe for training purposes
        traininnerdf = innerdf.join(studyidstrain, on=['STUDYID'], how='inner')
        testinnerdf = testinnerdf.where(testinnerdf['DAYS_INDEX_DX'] > testinnerdf['DAYS_INDEX'])
        traininnerdf = traininnerdf.where(traininnerdf['DAYS_INDEX_DX'] > traininnerdf['DAYS_INDEX'])
        testinnerdf = testinnerdf.select(testinnerdf['STUDYID'], testinnerdf['CODE'], testinnerdf['DAYS_INDEX'], testinnerdf['Diagnosed'])
        traininnerdf = traininnerdf.select(traininnerdf['STUDYID'], traininnerdf['CODE'], traininnerdf['DAYS_INDEX'], traininnerdf['Diagnosed'])
        if (i == 0):
            #Create a new dataframe
            testinnerdf.write.csv(str(sys.argv[1]) + '_clinvar_dataframe_test.csv', header="true", mode="overwrite")
            traininnerdf.write.csv(str(sys.argv[1]) + '_clinvar_dataframe_train.csv', header="true", mode="overwrite")
        else:
            #Append to existing dataframes
            testinnerdf.write.csv(str(sys.argv[1]) + '_clinvar_dataframe_test.csv', header="true", mode="append")
            traininnerdf.write.csv(str(sys.argv[1]) + '_clinvar_dataframe_train.csv', header="true", mode="append")
        i = i + 1
    spark.stop()

if __name__ == '__main__':
    func()
