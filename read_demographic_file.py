import sys
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql.functions import lit, concat, min, floor, concat_ws, collect_list, max, regexp_replace
from pyspark.ml.feature import QuantileDiscretizer
from pyspark.sql.types import *

def ReturnQuartile(df, cols):
    #This function groups a column into quartiles.  The input is cols, which is a list of columns
    for c in cols:
        #Create the quartiles
        discretizer = QuantileDiscretizer(numBuckets = 4, inputCol = c, outputCol = c + "bin", relativeError=0.01, handleInvalid = "error")
        #Apply the quartiles
        df = discretizer.fit(df).transform(df)
    return df

def func():
    spark = SparkSession.builder.appName("clin_var").getOrCreate()
    sc = spark.sparkContext
    df = spark.read.option("header", "true").option("inferschema", "true").csv("/mnt/data0/input/t2d_subgroups/t2d_patients.csv")
    #Read in the study ids generated by read_diagnosis_data
    studyidstest = spark.read.option("header", "true").option("inferschema", "true").csv(str(sys.argv[1]) + "_studyids_test.csv")
    studyidstrain = spark.read.option("header", "true").option("inferschema", "true").csv(str(sys.argv[1]) + "_studyids_train.csv")
    #Read the limit generated by read_diagnosis_data and conver tto float
    limitfile = open(str(sys.argv[1]) + "limitfile.txt", "r")
    limit = limitfile.read()
    limit = float(limit)
    limitfile.close()
    #Read the median generated by read_diagnosis_data and convert to float
    df = df.withColumn("INDEX_AGE", df["INDEX_AGE"].cast(DoubleType()))
    #these are the "important variables" that Scott defined
    importantvars = ["GENDER", "RACE", "INDEX_AGEbin"]
    #Quartile the variables
    df = ReturnQuartile(df, ["INDEX_AGE"])
    df = df.withColumn("INDEX_AGEbin", concat(df["INDEX_AGEbin"], lit("_AGE")))
    i = 0
    #This loop applies the results of qdf to the dataframe
    while (i < len(importantvars)):
        #Get the not null values
        innerdf = df.where(df[importantvars[i]].isNotNull())
        #Create the variables
        innerdf = innerdf.select(innerdf["STUDYID"], innerdf[importantvars[i]].alias("CODE"))
        innerdf = innerdf.where(innerdf['CODE'] != 'UNKNOWN/NOT DOCUMENTED')
        innerdf = innerdf.withColumn('DAYS_INDEX', lit(0))
        innerdf = innerdf.withColumn('CODE', regexp_replace(innerdf['CODE'], " ", "_"))
        testinnerdf = innerdf.join(studyidstest, on=['STUDYID'], how='inner')
        #Get the dataframe for training purposes
        traininnerdf = innerdf.join(studyidstrain, on=['STUDYID'], how='inner')
        testinnerdf = testinnerdf.select(testinnerdf['STUDYID'], testinnerdf['CODE'], testinnerdf['DAYS_INDEX'], testinnerdf['Diagnosed'])
        traininnerdf = traininnerdf.select(traininnerdf['STUDYID'], traininnerdf['CODE'], traininnerdf['DAYS_INDEX'], traininnerdf['Diagnosed'])
        if (i == 0):
            #Create a new dataframe
            testinnerdf.write.csv(str(sys.argv[1]) + '_demographics_dataframe_test.csv', header="true", mode="overwrite")
            traininnerdf.write.csv(str(sys.argv[1]) + '_demographics_dataframe_train.csv', header="true", mode="overwrite")
        else:
            #Append to existing dataframes
            testinnerdf.write.csv(str(sys.argv[1]) + '_demographics_dataframe_test.csv', header="true", mode="append")
            traininnerdf.write.csv(str(sys.argv[1]) + '_demographics_dataframe_train.csv', header="true", mode="append")
        i = i + 1
    spark.stop()

if __name__ == '__main__':
    func()
